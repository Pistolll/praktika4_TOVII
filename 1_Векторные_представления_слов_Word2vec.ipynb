{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pistolll/praktika4_TOVII/blob/main/1_%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D1%8B%D0%B5_%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F_%D1%81%D0%BB%D0%BE%D0%B2_Word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuM9XvCC23uF"
      },
      "source": [
        "# Векторные представления слов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLm3meyF23uR"
      },
      "source": [
        "## Введение\n",
        "\n",
        "Обработка последовательностей это одна из самых востребованных задач. Многие данные представляются последовательностями, звуки, тексты, видео и др. Обработка текстов, как последовательностей, дала мощный импульс в развитии методов обработки последовательностей. Многие архитектуры из обработки текстов применяются и для обработки звуков.\n",
        "\n",
        "Вспомним тему о векторных представлениях изображений. Мы увидели, что изображение можно описать массивом чисел - выходом какого-то из слоев нейронной сети - настолько точно, что по этому массиву можно провести классификацию изображения, отличить одно от другого.\n",
        "\n",
        "Раз можно так делать для изображений, то можно и для других типов данных, для звуков, сигналов и, конечно, для текстов.\n",
        "\n",
        "Раньше мы использовали векторные представления внутри нейронной сети, чтобы проводить классификацию, но оказывается, векторные представления интересны и сами по себе.\n",
        "\n",
        "Давайте введем простые векторные представления для текстов и посмотрим, какие у них получаются интересные свойства.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1FhgOr04D32"
      },
      "source": [
        "# О посимвольном представлении текста\n",
        "Несомненно, текст записывается как последовательность символов. Но человек обычно не обрабатывает каждый символ отдельно, а воспринимает \"слова\" целиком.\n",
        "\n",
        "Сможете ли вы прочитать этот [текст](https://habr.com/ru/articles/148896/) ?\n",
        "\n",
        " **По рзелульаттам илссеовадний одонго анлигйсокго унвиертисета, не иеемт занчнеия, в кокам пряокде рсапожолены бкувы в солве. Галвоне, чотбы преавя и пслоендяя бквуы блыи на мсете. Осатьлыне бкувы мгоут селдовтаь в плоонм бсепордяке, все-рвано ткест чтаитсея без побрелм. Пичрионй эгото ялвятеся то, что мы чиатем не кдаужю бкуву по отдльенотси, а все солво цликеом.**\n",
        "\n",
        "В английском варианте это звучало так:\n",
        "\n",
        "**Arocdnicg to rsceearch at Cmabrigde Uinervtisy, it deosn’t mttaer in waht oredr the ltteers in a wrod are, the olny iprmoatnt tihng is taht the frist and lsat ltteer are in the rghit pcale. The rset can be a toatl mses and you can sitll raed it wouthit pobelrm. Tihs is buseace the huamn mnid deos not raed ervey lteter by istlef, but the wrod as a wlohe.**\n",
        "\n",
        "Прочитали? А теперь всмотритесь в буквы.\n",
        "\n",
        "А такая [последовательность](https://www.inpearls.ru/)?\n",
        "\n",
        "**Варкалось. Хливкие шорьки\n",
        "Пырялись по наве,\n",
        "И хрюкотали зелюки,\n",
        "Как мюмзики в мове.**\n",
        "\n",
        "Вы понимаете смысл этого текста, можете провести его анализ.\n",
        "\n",
        "Таким образом, текст выгодней представлять не как последовательность символов, а как последовательность слов.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psYi8z3h23uT"
      },
      "source": [
        "# Унитарное кодирование\n",
        "Пожалуй самым простым векторным представлением является *унитарное кодирование* (по-английски one-hot encoding).\n",
        "\n",
        "Возьмем какой-либо текст, разобьем его на слова-элементы и составим словарь таких слов, в котором каждому слову будет приписан его номер в словаре.\n",
        "\n",
        "Этот номер слова можно представить в бинарном виде, сопоставим каждому слову в словаре *бинарный вектор*, состоящий из нулей и только одной единицы. Положение единицы в векторе показывает номер слова.\n",
        "\n",
        "![img](https://drive.google.com/uc?id=1hDY2EKB7ND-Uxq9FnKVtxC-U-0bnuiTe)\n",
        "\n",
        "В векторе будет столько элементов, сколько слов в словаре мы записали.\n",
        "\n",
        "Обратите внимание, что единичка, по сути, является признаком слова: стоит единичка в пятом элементе, значит слово номер пять.\n",
        "\n",
        "Вообще, в словарь мы можем добавлять какие угодно \"слова\", это могут быть обычные привычные нам слова, части слов и даже отдельные буквы, словосочетания и целые предложения, знаки препинания, смайлики, и прочее. Главное, что у всех этих \"слов\" есть номер, представленный унитарным вектором.\n",
        "\n",
        "Текст является последовательностью слов, а в унитарном представлении - последовательностью унитарных векторов.\n",
        "\n",
        "![img](https://drive.google.com/uc?id=1uEodrFkeG433O_zCWf3EpR8JVPYEFysx)\n",
        "\n",
        "Унитарное кодирование используется и для других типов информации, для изображений, сигналов и пр. Хорошим свойством является то, что унитарное представление не зависит от длины слова, ведь в словарь мы можем добавлять слова разной длины, хоть из одной буквы, хоть целое словосочетание.\n",
        "\n",
        "Однако такое представление не дает нам никаких новых интересных свойств, ведь унитарные вектора никак не связаны между собой и зависят от нашего произвола - в каком порядке слова в словарь добавляем, такой вектор и получим.\n",
        "\n",
        "Поэтому создают другие, более интересные, векторные представления."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_7KaRdJ23uT"
      },
      "source": [
        "# Word2vec\n",
        "Хочется как-то связать между собой слова и их векторные представления, ведь в реальных текстах слова, конечно же, связаны между собой **смыслом**. Вряд ли кто-то захочет читать тексты из случайных слов.\n",
        "\n",
        "Если есть взаимосвязь между словами текста, то ее можно уловить, вычислить.\n",
        "\n",
        "А давайте попробуем сделать так:\n",
        "- возьмем из текста три слова подряд.\n",
        "- закроем одно слово и попробуем его угадать по двум оставшимся.\n",
        "\n",
        "Во многих случаях, но конечно не всегда, это нам удастся. Попробуйте угадать какое слово скрыто под звездочками во фразе \"Мама мыла ....\".\n",
        "Если вы сказали \"раму\", то угадали, но могли сказать \"пол\" и не угадали.\n",
        "\n",
        "Давайте возьмем больше слов \"Мама мыла оконную ...\": тут легче угадать слово \"раму\", но все равно есть и другие варианты (придумайте).\n",
        "\n",
        "И все же, для многих последовательностей слов можно угадать какое слово закрыто, так пусть этим займется компьютер.\n",
        "\n",
        "Текст это последовательность слов, слова можно представить унитарными векторами. Нам надо лишь по нескольким унитарным векторам вычислять следующий. Это абсолютно то же самое, как если бы мы хотели обучить нейронную сеть. Известны входы - несколько унитарных векторов, знаем выход - один унитарный вектор, давайте обучим.\n",
        "\n",
        "Можно посмотреть и с другой стороны, если по нескольким словам мы угадываем следующее, то давайте наоборот, по слову угадаем какие слова ему предшествовали. Технически задача такая же. Знаем вход - один унитарный вектор, знаем выход - несколько унитарных векторов - так давайте обучим нейронную сеть.\n",
        "\n",
        "Эти подходы можно совместить, по нескольким векторам угадываем один пропущенный, а затем по нему угадываем обратно какие вектора были изначально, или, другими словами, угадываем слово по словам его окружающим, а затем по этому слову угадываем окружающие слова. Обычно угадывают среднее слово из трех, но это вовсе не обязательно.\n",
        "\n",
        "Такая технология получила название **word2vec** (произносится \"ворд ту век\"). На картинке показана для пяти слов, обозначенных буквой V с индексом в скобках.  Первую половинку - угадывание слова по окружению - назвали CBOW, вторую - угадывания окружения по слову - SkipGram.\n",
        "\n",
        "![img](https://drive.google.com/uc?id=18pa20uv6xi8tGC6pdOS3Vc11KUUcpW2e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeHc1Oxn23uU"
      },
      "source": [
        "Посмотрим на CBOW (это сокращение от Continuous Bag of Word, непрерывный мешок слов).\n",
        "\n",
        "Мы можем использовать простую нейронную сеть только с одним скрытым слоем для него.\n",
        "\n",
        "Унитарные вектора слов-окружения переводятся в скрытом слое в некоторые выходы, а затем по выходам этого скрытого слоя считается унитарный вектор исходного слова.\n",
        "\n",
        "Перевод унитарного представления слов-окружения в скрытом слое производится, как мы понимаем из работы нейронной сети, умножением на матрицу весов W. Ее размер (число нейронов в скрытом слое) * (число слов в словаре). Нет необходимости делать эту матрицу разной для разных слов-окружений, пусть будет одна.\n",
        "\n",
        "Аналогично, переход из скрытого слоя на выход также делается с помощью умножения на матрицу W'. Ее размер (число слов в словаре) * (число нейронов в скрытом слое).\n",
        "\n",
        "Матрица W' нам не важна, но посмотрим более внимательно на матрицу W.\n",
        "\n",
        "Она умножается на унитарный вектор, т.е. вектор, в котором все элементы нули кроме одного. Что произойдет при таком умножении?\n",
        "\n",
        "Правильно, выберется только один столбец из матрицы, с таким номером, на какой позиции стоит единичка во входом векторе. Но положение этой единички означает номер слова в словаре! Значит, каждому слову из словаря соответствует свой вектор-столбец в матрице W. Его размерность определяется числом нейронов в скрытом слое, которое мы задаем сами при обучении. **Эти вектора-столбцы и являются новым векторным представлением слов**.   \n",
        "\n",
        "Итак, вкратце, еще раз.\n",
        "- Берем тексты\n",
        "- разбиваем их на последовательность слов\n",
        "- переводим слова в унитарные вектора\n",
        "- на парах (соседние слова)-(среднее слово) обучаем простую нейронную сеть CBOW с ее матрицей W.\n",
        "- используем столбцы матрицы W как новое векторное представление слов. Длина этих векторов задается произвольно при обучении.\n",
        "\n",
        "![img](https://drive.google.com/uc?id=1bTpfqH0niwJKwMRHKdNbuOFiR9jucCZe)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqo4y3v523uV"
      },
      "source": [
        "# Геометрия слов\n",
        "Создав векторные представления для слов, можно их использовать в различных *геометрических* операциях: сложить вектора, вычесть, найти угол меду ними.\n",
        "\n",
        "И, что наиболее удивительно в таких векторных представлениях, эти геометрические операции с векторами имеют **смысл** с точки зрения самих слов.\n",
        "\n",
        "Например:\n",
        "* Король - мужчина + женщина = Королева\n",
        "* Великобритания - Лондон + Москва = Россия\n",
        " и др.\n",
        "\n",
        " Прежде чем смотреть на примеры, несколько слов о сравнении векторов.\n",
        "\n",
        "## Косинусное расстояние.\n",
        "\n",
        "Вектора можно сравнивать между собой, логично, что близкие вектора означают близкие по смыслу слова. Но как именно сравнивать вектора?\n",
        "\n",
        "Оказалось, что простое Евклидово расстояние между векторами не так интересно, более интересно сравнивать угол между векторами или, правильнее, косинус угла.\n",
        "\n",
        "Ну-ка, вспоминайте геометрию, как посчитать косинус угла между векторами?\n",
        "\n",
        "А вот так: скалярное произведение векторов поделить на длины этих векторов.\n",
        "\n",
        "![img](https://drive.google.com/uc?id=1vxVWdSAjY5oK6U7CtGadB0-2T5YLFmSh)\n",
        "\n",
        "Часто векторные представления *нормализуют*, т.е. приводят к единичной длине, тогда ее и считать не надо.\n",
        "\n",
        "Итак, мера схожести векторов - косинус угла между ними. Близкие по смыслу слова скорей всего дадут близкие по углу вектора."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BOipolf23uW"
      },
      "source": [
        "# Библиотека gensim\n",
        "\n",
        "А теперь примеры. Поиграть с векторами онлайн вы можете [здесь](https://rare-technologies.com/word2vec-tutorial/#app), но давайте и сами реализуем.  \n",
        "\n",
        "Нам поможет библиотека [gensim](https://radimrehurek.com/gensim/index.html). Установим ее.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка пакетов (после сброса среды!)\n",
        "!pip install gensim numpy\n",
        "\n",
        "# Импорты\n",
        "import logging\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "# Загружаем готовую векторную модель (маленькую)\n",
        "wv = api.load('glove-wiki-gigaword-50')\n",
        "\n",
        "# Пример: аналогия\n",
        "print(\"king - man + woman =\", wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "714rE6VPGX3v",
        "outputId": "0d794431-7af5-477d-8dcc-5e36e1dbb6c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "king - man + woman = [('queen', 0.8523604273796082)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXyn2JWv23uW"
      },
      "outputs": [],
      "source": [
        "# уже есть в Colab\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0J6XlEf23uY"
      },
      "source": [
        "Подключим вспомогательную библиотеку для записи действий."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PtqCCWh423uY"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2HHRtF923uZ"
      },
      "source": [
        "Скачаем уже обученную модель [word2vec](https://radimrehurek.com/gensim/models/word2vec.html), она была обучена на огромном наборе текстов на английском языке из 3 млн. слов и занимает около 2 Гигабайт, придется подождать. Это надо сделать только один раз. Модель загружается в виде специального объекта, в котором прописаны многие методы для работы с векторами.\n",
        "\n",
        "*Для учителя: иногда сайт обрывает связь и закачку, рекомендуется заранее скачать массивы используя методы load(), save(), или используйте меньшие вектора, или скачайте заранее и поместите в директорию /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zw26YKfY23ua"
      },
      "outputs": [],
      "source": [
        "#import gensim.downloader as api #\n",
        "#wv = api.load('word2vec-google-news-300') # большая модель\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptoeU-ro-YHI"
      },
      "source": [
        "Воспользуемся корпусом поменьше и обучим на нем модель word2vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f_q5qYCi3txa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "641551d1-437a-46e3-9cb3-eda51215ca5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
          ]
        }
      ],
      "source": [
        "# Корпус поменьше\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "corpus = api.load('text8') # загружаем корпус текстов\n",
        "model = Word2Vec(corpus) # обучаем модель ~ 5 минут\n",
        "wv=model.wv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prcvk_j023ua"
      },
      "source": [
        "Выведем на экран первые 100 слов из словаря, которые хранятся в поле .index_to_key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeBsRaP_23ub",
        "outputId": "6e1ba950-89b0-44d9-a7ba-78c720a0967d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word #0/71290 is the\n",
            "word #1/71290 is of\n",
            "word #2/71290 is and\n",
            "word #3/71290 is one\n",
            "word #4/71290 is in\n",
            "word #5/71290 is a\n",
            "word #6/71290 is to\n",
            "word #7/71290 is zero\n",
            "word #8/71290 is nine\n",
            "word #9/71290 is two\n",
            "word #10/71290 is is\n",
            "word #11/71290 is as\n",
            "word #12/71290 is eight\n",
            "word #13/71290 is for\n",
            "word #14/71290 is s\n",
            "word #15/71290 is five\n",
            "word #16/71290 is three\n",
            "word #17/71290 is was\n",
            "word #18/71290 is by\n",
            "word #19/71290 is that\n",
            "word #20/71290 is four\n",
            "word #21/71290 is six\n",
            "word #22/71290 is seven\n",
            "word #23/71290 is with\n",
            "word #24/71290 is on\n",
            "word #25/71290 is are\n",
            "word #26/71290 is it\n",
            "word #27/71290 is from\n",
            "word #28/71290 is or\n",
            "word #29/71290 is his\n",
            "word #30/71290 is an\n",
            "word #31/71290 is be\n",
            "word #32/71290 is this\n",
            "word #33/71290 is which\n",
            "word #34/71290 is at\n",
            "word #35/71290 is he\n",
            "word #36/71290 is also\n",
            "word #37/71290 is not\n",
            "word #38/71290 is have\n",
            "word #39/71290 is were\n",
            "word #40/71290 is has\n",
            "word #41/71290 is but\n",
            "word #42/71290 is other\n",
            "word #43/71290 is their\n",
            "word #44/71290 is its\n",
            "word #45/71290 is first\n",
            "word #46/71290 is they\n",
            "word #47/71290 is some\n",
            "word #48/71290 is had\n",
            "word #49/71290 is all\n",
            "word #50/71290 is more\n",
            "word #51/71290 is most\n",
            "word #52/71290 is can\n",
            "word #53/71290 is been\n",
            "word #54/71290 is such\n",
            "word #55/71290 is many\n",
            "word #56/71290 is who\n",
            "word #57/71290 is new\n",
            "word #58/71290 is used\n",
            "word #59/71290 is there\n",
            "word #60/71290 is after\n",
            "word #61/71290 is when\n",
            "word #62/71290 is into\n",
            "word #63/71290 is american\n",
            "word #64/71290 is time\n",
            "word #65/71290 is these\n",
            "word #66/71290 is only\n",
            "word #67/71290 is see\n",
            "word #68/71290 is may\n",
            "word #69/71290 is than\n",
            "word #70/71290 is world\n",
            "word #71/71290 is i\n",
            "word #72/71290 is b\n",
            "word #73/71290 is would\n",
            "word #74/71290 is d\n",
            "word #75/71290 is no\n",
            "word #76/71290 is however\n",
            "word #77/71290 is between\n",
            "word #78/71290 is about\n",
            "word #79/71290 is over\n",
            "word #80/71290 is years\n",
            "word #81/71290 is states\n",
            "word #82/71290 is people\n",
            "word #83/71290 is war\n",
            "word #84/71290 is during\n",
            "word #85/71290 is united\n",
            "word #86/71290 is known\n",
            "word #87/71290 is if\n",
            "word #88/71290 is called\n",
            "word #89/71290 is use\n",
            "word #90/71290 is th\n",
            "word #91/71290 is system\n",
            "word #92/71290 is often\n",
            "word #93/71290 is state\n",
            "word #94/71290 is so\n",
            "word #95/71290 is history\n",
            "word #96/71290 is will\n",
            "word #97/71290 is up\n",
            "word #98/71290 is while\n",
            "word #99/71290 is where\n"
          ]
        }
      ],
      "source": [
        "for index, word in enumerate(wv.index_to_key):\n",
        "    if index == 100:\n",
        "        break\n",
        "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqjsNfe223ub"
      },
      "source": [
        "Получить вектор слова можно используя это слово как индекс. Получим для слова 'king'. Это вектор из 100 элементов. Конечно, это слово должно быть в словаре и именно в таком виде, иначе возникнет ошибка. Важен и регистр букв, попробуйте слово 'kinG'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7rmj06m23uc",
        "outputId": "85d7dd9b-f270-4ade-e4fd-f6ceb19acdda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "vec_king = wv['king'] # такое слово есть в словаре\n",
        "vec_king.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V3CduHpM_u80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "5a602c47-020d-4705-d83f-b9d82139b589"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Key 'kinG' not present\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-da8dfe1e7e4e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvec_king\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kinG'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# такого слова нет в словаре\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \"\"\"\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \"\"\"\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'kinG' not present\""
          ]
        }
      ],
      "source": [
        "vec_king = wv['kinG'] # такого слова нет в словаре"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uzk2QWmI23uc"
      },
      "source": [
        "Посчитать \"похожесть\" слов, т.е. их векторов, можно с помощью метода `.similarity()`. Чем больше число, тем более похожи слова."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvdK5kQR23uc",
        "outputId": "5b3f7731-d47e-44b6-e0cf-6f7f9d1ab95e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'car'\t'minivan'\t0.25\n",
            "'car'\t'bicycle'\t0.57\n",
            "'car'\t'airplane'\t0.55\n",
            "'car'\t'cereal'\t0.15\n",
            "'car'\t'communism'\t-0.17\n"
          ]
        }
      ],
      "source": [
        "pairs = [                 # пары слов\n",
        "    ('car', 'minivan'),   # минивэн это тип автомобиля\n",
        "    ('car', 'bicycle'),   # мотоцикл имеет колеса как и автомобиль\n",
        "    ('car', 'airplane'),  # ладно, самолет не колесное средство, но все же средство передвижения\n",
        "    ('car', 'cereal'),    # зерно и автомобиль, хм..\n",
        "    ('car', 'communism'), # какая связь между автомобилем и коммунизмом???\n",
        "]\n",
        "for w1, w2 in pairs:\n",
        "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huAtz1iv23ud"
      },
      "source": [
        "Перебирать вручную все пары слов, чтобы найти самые похожие будет трудно, пусть компьютер сравнит все-все вектора и выведет самые похожие на заданный.\n",
        "\n",
        "Метод `.most_similar()` принимает набор слов для которых искать похожие (аргумент positive) и число похожих слов (аргумент topn)\n",
        "\n",
        "(Первый запуск может быть долгим, так как надо посчитать длины всех векторов, потом будет гораздо быстрее)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29vJFCWf23ud",
        "outputId": "c32de12c-3e3e-434e-e250-ad3d49cbbb1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('girl', 0.7645989656448364), ('child', 0.7028816938400269), ('person', 0.6606707572937012), ('gentleman', 0.6596999764442444), ('bride', 0.6582705974578857)]\n",
            "[('hamster', 0.7471149563789368), ('bird', 0.6797839999198914), ('breed', 0.6619564890861511), ('goat', 0.6360880732536316), ('ass', 0.6316744089126587)]\n"
          ]
        }
      ],
      "source": [
        "print(wv.most_similar(positive=['man', 'woman'], topn=5))\n",
        "print(wv.most_similar(positive=['cat','dog'],negative=['duck'], topn=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av4fcfol23ue"
      },
      "source": [
        "Найти слово, которое непохоже на остальные? Легко, метод .doesnt_match(). Все слова (их вектора) будут сравнены между собой и выведется то, которое похоже меньше всех."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYnqSCA823ue",
        "outputId": "342da011-88e3-4994-bc04-088eaffce594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car\n"
          ]
        }
      ],
      "source": [
        "print(wv.doesnt_match(('fire', 'water', 'land', 'sea', 'air', 'car')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwgLBaTM23ue"
      },
      "source": [
        "Геометрические операции.\n",
        "\n",
        "Вектор 'France' минус вектор 'Paris' плюс вектор 'Moscow'. Получится какой-то вектор. Его может не быть в словаре, найдем ближайшие из словаря к нему, метод `.similar_by_vector()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCYoxEHB23ue",
        "outputId": "185eec85-4d60-45a4-a4d1-ed2b406093e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('finland', 0.7049514055252075),\n",
              " ('kuwait', 0.6967581510543823),\n",
              " ('libya', 0.6960716843605042),\n",
              " ('lithuania', 0.6892430186271667),\n",
              " ('russia', 0.6874240636825562),\n",
              " ('moldova', 0.6825775504112244),\n",
              " ('afghanistan', 0.6737417578697205),\n",
              " ('cyprus', 0.671637237071991),\n",
              " ('albania', 0.6589299440383911),\n",
              " ('indonesia', 0.6581672430038452)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "Russia=wv['france']-wv['paris']+wv['moscow']\n",
        "wv.similar_by_vector(Russia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GifuP9hR1-_",
        "outputId": "72bd3051-0a08-4b8b-94b5-1b9b8b99778c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('deficient', 0.565294623374939),\n",
              " ('confusing', 0.5636296272277832),\n",
              " ('transitive', 0.5618667602539062),\n",
              " ('conversely', 0.5601799488067627),\n",
              " ('reflexive', 0.553476095199585),\n",
              " ('chiral', 0.5425931215286255),\n",
              " ('cooked', 0.5369694828987122),\n",
              " ('dim', 0.533639669418335),\n",
              " ('reactive', 0.5320882201194763),\n",
              " ('gut', 0.5319165587425232)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "q=wv['drunk']-wv['was']+wv['is']\n",
        "wv.similar_by_vector(q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QS6pGzd23ue"
      },
      "source": [
        "А на русском языке??\n",
        "\n",
        "Такие модели тоже есть, но они гораздо слабее, мало слов, мало текстов.\n",
        "Загрузим модель word2vec-ruscorpora-300 . Всего-то 200 тысяч слов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOWgBWZa23ug",
        "outputId": "ee8900e5-74ab-4a16-f93a-fc328a5ea1d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 198.8/198.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "wv_rus = api.load('word2vec-ruscorpora-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTcErjx923ug"
      },
      "source": [
        "Здесь слова имеют приставки, показывающие их часть речи."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "175L8SiI23ug"
      },
      "source": [
        "что получится для король - мужчина + женщина ?\n",
        "По идее королева. Но получится король. Но и \"правильный\" ответ тоже недалеко, второй по счету.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBtwTl2n23ug",
        "outputId": "f8fd5433-9f64-4bcd-b2d7-b64c1107879f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('король_NOUN', 0.8805387616157532),\n",
              " ('королева_NOUN', 0.7313904762268066),\n",
              " ('герцог_NOUN', 0.6502388715744019),\n",
              " ('принцесса_NOUN', 0.6266285181045532),\n",
              " ('герцогиня_NOUN', 0.6240381598472595),\n",
              " ('королевство_NOUN', 0.6094207167625427),\n",
              " ('зюдерманландский_ADJ', 0.6084389686584473),\n",
              " ('дурлахский_ADJ', 0.6081665754318237),\n",
              " ('ульрик::элеонора_NOUN', 0.6073107719421387),\n",
              " ('максимилианов_NOUN', 0.6057003736495972)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "queen= wv_rus['король_NOUN'] - wv_rus['мужчина_NOUN'] + wv_rus['женщина_NOUN']\n",
        "\n",
        "wv_rus.similar_by_vector(queen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuD6ipd223uh"
      },
      "source": [
        "# Задания\n",
        "Успешность геометрических операций с векторами слов сильно зависит и от вида модели и от текстов, на которых она обучалась. Не все понятные человеку аналогии слов, получаются и в компьютере. В качестве упражнения попробуйте найти примеры логичных и нелогичных аналогий."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_most_similar(positive, negative):\n",
        "    # Проверяем, есть ли все слова в словаре\n",
        "    for word in positive + negative:\n",
        "        if word not in model.wv:\n",
        "            print(f\"Слово '{word}' отсутствует в словаре модели\")\n",
        "            return None\n",
        "    return model.wv.most_similar(positive=positive, negative=negative)\n",
        "\n",
        "print(\"Логичные аналогии:\")\n",
        "print(safe_most_similar(positive=['король', 'женщина'], negative=['мужчина']))\n",
        "print(safe_most_similar(positive=['париж', 'италия'], negative=['франция']))\n",
        "print(safe_most_similar(positive=['москва', 'германия'], negative=['россия']))\n",
        "\n",
        "print(\"\\nНелогичные аналогии:\")\n",
        "print(safe_most_similar(positive=['стол', 'радость'], negative=['солнце']))\n",
        "print(safe_most_similar(positive=['река', 'библиотека'], negative=['машина']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFCijw6dEPfH",
        "outputId": "179e224f-4f38-4ebe-9192-9271160e1c00"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Логичные аналогии:\n",
            "Слово 'король' отсутствует в словаре модели\n",
            "None\n",
            "Слово 'париж' отсутствует в словаре модели\n",
            "None\n",
            "Слово 'москва' отсутствует в словаре модели\n",
            "None\n",
            "\n",
            "Нелогичные аналогии:\n",
            "Слово 'стол' отсутствует в словаре модели\n",
            "None\n",
            "Слово 'река' отсутствует в словаре модели\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Логичные аналогии:\")\n",
        "print(wv_rus.most_similar(positive=['король_NOUN', 'женщина_NOUN'], negative=['мужчина_NOUN']))\n",
        "print(wv_rus.most_similar(positive=['париж_NOUN', 'италия_NOUN'], negative=['франция_NOUN']))\n",
        "print(wv_rus.most_similar(positive=['москва_NOUN', 'германия_NOUN'], negative=['россия_NOUN']))\n",
        "\n",
        "print(\"\\nНелогичные аналогии:\")\n",
        "print(wv_rus.most_similar(positive=['стол_NOUN', 'радость_NOUN'], negative=['солнце_NOUN']))\n",
        "print(wv_rus.most_similar(positive=['река_NOUN', 'библиотека_NOUN'], negative=['машина_NOUN']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8lTFdNCJBSH",
        "outputId": "d920b8bc-ae27-4ede-f9b9-461f865ef9f2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Логичные аналогии:\n",
            "[('королева_NOUN', 0.7313904762268066), ('герцог_NOUN', 0.6502388715744019), ('принцесса_NOUN', 0.6266285181045532), ('герцогиня_NOUN', 0.6240381598472595), ('королевство_NOUN', 0.6094207167625427), ('зюдерманландский_ADJ', 0.6084389686584473), ('дурлахский_ADJ', 0.608166515827179), ('ульрик::элеонора_NOUN', 0.6073107123374939), ('максимилианов_NOUN', 0.6057004332542419), ('принц_NOUN', 0.5984029173851013)]\n",
            "[('ницца_NOUN', 0.6837807297706604), ('флоренция_NOUN', 0.6755390167236328), ('венеция_NOUN', 0.6641937494277954), ('неаполь_NOUN', 0.6582341194152832), ('милан_NOUN', 0.6489267349243164), ('рим_NOUN', 0.6479753851890564), ('лондон_NOUN', 0.6468997001647949), ('берлин_NOUN', 0.6394644975662231), ('генуя_NOUN', 0.6336957812309265), ('мюнхен_NOUN', 0.61598140001297)]\n",
            "[('берлин_NOUN', 0.7166863679885864), ('мюнхен_NOUN', 0.6141614317893982), ('гамбург_NOUN', 0.5910831689834595), ('дрезден_NOUN', 0.5751315951347351), ('бабельсберг_NOUN', 0.5680848360061646), ('париж_NOUN', 0.5643577575683594), ('лейпциг_NOUN', 0.555885910987854), ('карлсхорст_NOUN', 0.5532953143119812), ('билефельд_NOUN', 0.5470367074012756), ('дуйсбург_NOUN', 0.5449016690254211)]\n",
            "\n",
            "Нелогичные аналогии:\n",
            "[('столик_NOUN', 0.47733452916145325), ('огорчение_NOUN', 0.46257150173187256), ('стул_NOUN', 0.4325825273990631), ('поднос_NOUN', 0.43163979053497314), ('благодарность_NOUN', 0.4199906885623932), ('тумбочка_NOUN', 0.4132344424724579), ('мавр::савишну_NOUN', 0.411571741104126), ('диван_NOUN', 0.41059455275535583), ('удовольствие_NOUN', 0.4104997515678406), ('горница_NOUN', 0.4069534242153168)]\n",
            "[('речка_NOUN', 0.4649342894554138), ('устье_NOUN', 0.4488164484500885), ('кускоквим_NOUN', 0.44641774892807007), ('оксус_NOUN', 0.44421353936195374), ('озеро_NOUN', 0.43618908524513245), ('пихец_NOUN', 0.4330923855304718), ('немпт_NOUN', 0.4280956983566284), ('мухеня_NOUN', 0.42797356843948364), ('верховье_NOUN', 0.42490944266319275), ('хуан::хэ_NOUN', 0.42436087131500244)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "most_similar возвращает список слов из словаря модели, которые максимально близки по смыслу к вычисленному вектору (аналогии).\n",
        "\n"
      ],
      "metadata": {
        "id": "rIOpr9jPJ7RI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Логичные аналогии\n",
        "\n",
        "most_similar(positive=['король_NOUN', 'женщина_NOUN'], negative=['мужчина_NOUN'])\n",
        "выдал похожие слова:\n",
        "королева_NOUN (женский аналог короля)\n",
        "герцог_NOUN, принцесса_NOUN — родственные понятия из той же сферы\n",
        "и т.д.\n",
        "\n",
        "Это значит, что модель правильно \"понимает\" аналогию: король - мужчина + женщина ≈ королева."
      ],
      "metadata": {
        "id": "Imu9cwn8J8Vf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "То же с городами:\n",
        "\n",
        "Париж - Франция + Италия ≈ столица Италии (Рим и другие итальянские города)\n",
        "\n",
        "Москва - Россия + Германия ≈ крупные немецкие города (Берлин, Мюнхен)"
      ],
      "metadata": {
        "id": "BNo4Gxk8KCCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нелогичные аналогии\n",
        "most_similar(positive=['стол_NOUN', 'радость_NOUN'], negative=['солнце_NOUN'])\n",
        "модель выдала список слов, которые не имеют логической связи между собой и с запросом:\n",
        "\n",
        "столик_NOUN, огорчение_NOUN, стул_NOUN, поднос_NOUN и т.д.\n",
        "\n",
        "То есть модель пытается найти что-то похожее по векторному признаку, но в реальном языке это не имеет смысла.\n",
        "\n"
      ],
      "metadata": {
        "id": "oYGP6uKKKCkv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il0jKMxH23uh"
      },
      "source": [
        "# Заключение\n",
        "Сегодня существует множество моделей для векторных представлений слов на разных языках. По похожему принципу строятся и векторные представления предложений и даже текстов целиком. Мы еще познакомимся с некоторыми примерами в этой области."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHQmJaAJ23ui"
      },
      "source": [
        "## Ссылки\n",
        "\n",
        "Использованы и адаптированы материалы:\n",
        "* https://radimrehurek.com/gensim/index.html\n",
        "* https://medium.com/sciforce/word-vectors-in-natural-language-processing-global-vectors-glove-51339db89639\n",
        "* https://colab.research.google.com/github/mdda/deep-learning-workshop/blob/master/notebooks/5-RNN/3-Text-Corpus-and-Embeddings.ipynb#scrollTo=h-CQETk6HPmx\n",
        "* https://radimrehurek.com/gensim/models/fasttext.html\n",
        "* https://amitness.com/2020/06/fasttext-embeddings/\n",
        "* http://vectors.nlpl.eu/explore/embeddings\n",
        "\n",
        "Рекомендую посмотреть курсы:\n",
        "* First Step in NLP https://stepik.org/course/129443/syllabus\n",
        "* Second Step in NLP https://stepik.org/course/133963/syllabus"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}